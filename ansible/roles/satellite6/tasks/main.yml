# gluster 2-node kvm-based demo playbook
# Jamie Duncan - jduncan@redhat.com

- name: disable all repositories so we can enable just the ones we want
  command: subscription-manager repos --disable=\*

- name: enable the needed rhel repositories
  command: subscription-manager repos --enable={{ item }}
  with_items: "{{ repo_channels }}"

- name: install glusterd
  package: name=glusterfs-server state=present

- name: update the server
  package: name=* state=latest

- name: reboot each system
  shell: sleep 2 && systemctl reboot
  async: 1
  poll: 0
  ignore_errors: true

- name: wait for server to come back
  local_action: wait_for host={{ inventory_hostname }} state=started delay=30 timeout=300 port=22
  sudo: false

- name: create vg and pv for gluster
  lvg: vg={{ vg_name }} pvs=/dev/vdb state=present

- name: create logical volumes for gluster bricks
  lvol: vg=vg_gluster lv={{ item }} size={{ brick_size }}
  with_items: "{{ gluster_bricks }}"

- name: create xfs filesystem for each brick
  filesystem: fstype=xfs dev=/dev/{{ vg_name }}/{{ item }}
  with_items: "{{ gluster_bricks }}"

- name: create bricks directory
  file: path=/bricks state=directory mode=0755

- name: create directories for brick mountpoints
  file: path=/bricks/{{ item }} state=directory mode=0755
  with_items: "{{ gluster_bricks }}"

- name: mount the xfs volumes for each brick
  mount: name=/bricks/{{ item }} src=/dev/{{ vg_name }}/{{ item }} fstype=xfs state=mounted
  with_items: "{{ gluster_bricks }}"

- name: create a containers subdirectory for each volume
  file: path=/bricks/{{ item }}/containers state=directory mode=0755
  with_items: "{{ gluster_bricks }}"

- name: start and enable glusterd
  service: name=glusterd state=started enabled=yes
  register: glusterd_status

- name: wait for glusterd to start up
  pause: seconds=30
  when: glusterd_status.changed

- name: probe for gluster1 from gluster0
  command: gluster peer probe gluster1.example.com
  when: role == 'master'

- name: check status of gluster peers
  command: gluster peer status
  when: role == 'master'
  register: gluster_peer_status

- name: create the gluster volume
  command: gluster volume create container_registry replica 2 transport tcp gluster0.example.com:/bricks/brick1/containers gluster1.example.com:/bricks/brick1/containers
  #gluster_volume: state=present name=container_registry bricks=/bricks/brick1/containers rebalance=yes cluster="192.168.122.160,192.168.122.161"
  when: role == 'master' and "'Peer in Cluster' in gluster_peer_status.stdout"

- name: start the gluster volume
  command: gluster volume start container_registry
  when: role == 'master' and "'Peer in Cluster' in gluster_peer_status.stdout"
